{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Sufficiency Analysis for Classification Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from daml.datasets import DamlDataset\n",
    "from daml.metrics.sufficiency import Sufficiency\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "datasets.MNIST('./data', train=True, download=True)\n",
    "datasets.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST data and create the training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ds(data: Dataset, len: int) -> DamlDataset:\n",
    "    loader = DataLoader(Subset(data, range(len)), len)\n",
    "    return DamlDataset(next(iter(loader))[0].numpy(), next(iter(loader))[1].numpy())\n",
    "\n",
    "to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "train_ds = to_ds(datasets.MNIST('./data', train=True, download=True, transform=to_tensor), 2000)\n",
    "test_ds = to_ds(datasets.MNIST('./data', train=False, download=True, transform=to_tensor), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for lbl in range(10):\n",
    "    i = np.argwhere(train_ds.labels == lbl)[0]\n",
    "    img = np.reshape(train_ds.images[i], (28, 28))\n",
    "    ax = fig.add_subplot(2, 5, lbl+1)\n",
    "    ax.imshow(img, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the network architecture we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(6400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Compile the model and typecast for proper type hinting\n",
    "model = typing.cast(nn.Module, torch.compile(Net().to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define our custom training and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train(model: nn.Module, dataloader: DataLoader):\n",
    "    # Defined only for this testing scenario\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            X = torch.Tensor(batch[0]).to(device)\n",
    "            y = torch.Tensor(batch[1]).to(device)\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward Propagation\n",
    "            outputs = model(X)\n",
    "            # Back prop\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            # Update optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def custom_eval(model: nn.Module, dataloader: DataLoader) -> float:\n",
    "    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "    result = 0\n",
    "\n",
    "    # Set model layers into evaluation mode\n",
    "    model.eval()\n",
    "    # Tell PyTorch to not track gradients, greatly speeds up processing\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = torch.Tensor(batch[0]).to(device)\n",
    "            y = torch.Tensor(batch[1]).to(device)\n",
    "            preds = model(X)\n",
    "            metric.update(preds, y)\n",
    "        result = metric.compute()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize metric\n",
    "\n",
    "Attach the custom training and evaluation functions to the Sufficiency metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate sufficiency metric\n",
    "suff = Sufficiency()\n",
    "\n",
    "# Set training and eval functions defined above\n",
    "suff.set_training_func(custom_train)\n",
    "suff.set_eval_func(custom_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define training parameters\n",
    "\n",
    "Define the number of models to train in parallel (stability), as well as the number of steps along the learning curve to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(train_ds)\n",
    "model_count = 10\n",
    "num_steps = 10\n",
    "\n",
    "# Create data indices for training\n",
    "suff.setup(length, model_count, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Sufficiency\n",
    "\n",
    "Now we can run the metric to train the models and produce the learning curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & test model\n",
    "output = suff.run(model, train_ds, test_ds, batch_size=16)\n",
    "\n",
    "suff.plot(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "Using this learning curve, we can project performance under much larger datasets (with the same model)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
