{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Classification Dataset Sufficiency Analysis via Learning Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything we need\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "from daml.datasets import DamlDataset\n",
    "from daml.metrics.sufficiency import Sufficiency\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST data with load_dataset(), define network architecture, and define training and evaluation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that loads in MNIST data and creates a DAML dataset with it\n",
    "def load_dataset():\n",
    "    # Loads dataset\n",
    "    path = \"../../tests/datasets/mnist.npz\"\n",
    "    with np.load(path, allow_pickle=True) as fp:\n",
    "        images, labels = fp[\"x_train\"][:4000], fp[\"y_train\"][:4000]\n",
    "        test_images, test_labels = fp[\"x_test\"][:500], fp[\"y_test\"][:500]\n",
    "    images = images.reshape((4000, 1, 28, 28))\n",
    "    test_images = test_images.reshape((500, 1, 28, 28))\n",
    "    train_ds = DamlDataset(images, labels)\n",
    "    test_ds = DamlDataset(test_images, test_labels)\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "# Define our network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(6400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def custom_train(model: nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Passes data once through the model with backpropagation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The trained model that will be evaluated\n",
    "    X : torch.Tensor\n",
    "        The training data to be passed through the model\n",
    "    y : torch.Tensor\n",
    "        The training labels corresponding to the data\n",
    "    \"\"\"\n",
    "    # Defined only for this testing scenario\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    epochs = 5\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Propagation\n",
    "        outputs = model(X)\n",
    "        # Back prop\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        # Update optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def custom_eval(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a model on a single pass with a given metric\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The trained model that will be evaluated\n",
    "    X : torch.Tensor\n",
    "        The testing data to be passed through th model\n",
    "    y : torch.Tensor\n",
    "        The testing labels corresponding to the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The calculated performance of the model\n",
    "    \"\"\"\n",
    "    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    # Set model layers into evaluation mode\n",
    "    model.eval()\n",
    "    # Tell PyTorch to not track gradients, greatly speeds up processing\n",
    "    with torch.no_grad():\n",
    "        preds = model(X)\n",
    "        result = metric(preds, y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define daml sufficiency function, and attach custom training and evaluation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = load_dataset()\n",
    "model = Net()\n",
    "length = len(train_ds)\n",
    "\n",
    "# Instantiate sufficiency metric\n",
    "suff = Sufficiency()\n",
    "# Set predefined training and eval functions\n",
    "suff.set_training_func(custom_train)\n",
    "suff.set_eval_func(custom_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define number of models to train in parallel (stability), as well as the number of steps along the learning curve to evaluate. Train models to produce learning curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data indices for training\n",
    "m_count = 10\n",
    "num_steps = 10\n",
    "suff.setup(length, m_count, num_steps)\n",
    "# Train & test model\n",
    "output = suff.run(model, train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suff.plot(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this learning curve, we can project performance under much larger datasets (with the same model).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
