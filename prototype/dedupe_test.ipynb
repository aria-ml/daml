{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Data\n",
    "import hashlib\n",
    "import os\n",
    "import typing\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def download_mnist() -> str:\n",
    "    \"\"\"Code to download mnist originates from keras/datasets:\n",
    "\n",
    "    https://github.com/keras-team/keras/blob/v2.15.0/keras/datasets/mnist.py#L25-L86\n",
    "    \"\"\"\n",
    "    origin_folder = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n",
    "    path = _get_file(\n",
    "        \"mnist.npz\",\n",
    "        origin=origin_folder + \"mnist.npz\",\n",
    "        file_hash=(\"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"),\n",
    "    )\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def _get_file(\n",
    "    fname: str,\n",
    "    origin: str,\n",
    "    file_hash: typing.Optional[str] = None,\n",
    "):\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".keras\")\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join(\"/tmp\", \".keras\")\n",
    "    datadir = os.path.join(datadir_base, \"datasets\")\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fname = os.fspath(fname) if isinstance(fname, os.PathLike) else fname\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "        if file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        try:\n",
    "            error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg)) from e\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason)) from e\n",
    "        except (Exception, KeyboardInterrupt):\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "        if (\n",
    "            os.path.exists(fpath)\n",
    "            and file_hash is not None\n",
    "            and not _validate_file(fpath, file_hash)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Incomplete or corrupted file detected. \"\n",
    "                f\"The sha256 file hash does not match the provided value \"\n",
    "                f\"of {file_hash}.\",\n",
    "            )\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def _validate_file(fpath, file_hash, chunk_size=65535):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(fpath, \"rb\") as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return str(hasher.hexdigest()) == str(file_hash)\n",
    "\n",
    "\n",
    "mnist_path = download_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "rng = np.random.default_rng(33)\n",
    "size = 12\n",
    "path = download_mnist()\n",
    "with np.load(path, allow_pickle=True) as fp:\n",
    "    images, labels = fp[\"x_train\"][:size], fp[\"y_train\"][:size]\n",
    "\n",
    "dup_images = deepcopy(images[:8]).astype(\"float64\")\n",
    "dup_images[:, :25, :25] = images[:8, 3:, 3:]\n",
    "dup_images[:, 25:, 25:] = images[:8, :3, :3]\n",
    "\n",
    "test_imgs = np.concatenate([images, dup_images])\n",
    "test_imgs /= 255\n",
    "\n",
    "rng.shuffle(test_imgs)\n",
    "\n",
    "data = test_imgs.reshape((test_imgs.shape[0], -1))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(0, len(images))\n",
    "plt.imshow(images[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(arr):\n",
    "    max_clusters = 1\n",
    "    max_levels = 1\n",
    "    clusters = {}\n",
    "    for i in range(len(arr)):\n",
    "        level = 1\n",
    "        cluster_num = max_clusters\n",
    "        distance = 0\n",
    "        count = 0\n",
    "        sample_added = []\n",
    "        if arr[i, 0] in clusters:\n",
    "            cluster_num = min([cluster_num, clusters[arr[i, 0]][\"cluster_num\"]])\n",
    "            left_level = max([level, clusters[arr[i, 0]][\"level\"] + 1])\n",
    "            distance += clusters[arr[i, 0]][\"total_dist\"]\n",
    "            count += clusters[arr[i, 0]][\"count\"]\n",
    "        else:\n",
    "            sample_added.append(int(arr[i, 0]))\n",
    "\n",
    "        if arr[i, 1] in clusters:\n",
    "            cluster_num = min([cluster_num, clusters[arr[i, 1]][\"cluster_num\"]])\n",
    "            right_level = max([level, clusters[arr[i, 1]][\"level\"] + 1])\n",
    "            distance += clusters[arr[i, 1]][\"total_dist\"]\n",
    "            count += clusters[arr[i, 1]][\"count\"]\n",
    "        else:\n",
    "            sample_added.append(int(arr[i, 1]))\n",
    "\n",
    "        if arr[i, 0] in clusters and arr[i, 1] in clusters:\n",
    "            if cluster_num == clusters[arr[i, 0]][\"cluster_num\"]:\n",
    "                level = left_level\n",
    "            elif cluster_num == clusters[arr[i, 1]][\"cluster_num\"]:\n",
    "                level = right_level\n",
    "        elif arr[i, 0] in clusters:\n",
    "            level = left_level\n",
    "        elif arr[i, 1] in clusters:\n",
    "            level = right_level\n",
    "\n",
    "        count += 1\n",
    "        distance += arr[i, 2]\n",
    "\n",
    "        clusters[arr[i, -1]] = {\n",
    "            \"cluster_num\": cluster_num,\n",
    "            \"level\": level,\n",
    "            \"total_dist\": distance,\n",
    "            \"count\": count,\n",
    "            \"avg_dist\": distance / count,\n",
    "            \"samples_added\": sample_added,\n",
    "            \"sample_dist\": arr[i, 2],\n",
    "        }\n",
    "\n",
    "        if cluster_num == max_clusters and i < len(arr) - 1:\n",
    "            max_clusters += 1\n",
    "\n",
    "        if level > max_levels:\n",
    "            max_levels = level\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_opt(arr):\n",
    "    max_clusters = 1\n",
    "    max_levels = 1\n",
    "    clusters = {}\n",
    "\n",
    "    for i, arr_i in enumerate(arr):\n",
    "        level = 1\n",
    "        cluster_num = max_clusters\n",
    "\n",
    "        distance = 0\n",
    "        count = 0\n",
    "        new_sample = []\n",
    "\n",
    "        arr_0 = int(arr_i[0])\n",
    "        arr_1 = int(arr_i[1])\n",
    "        # print(arr_0, arr_1)\n",
    "\n",
    "        # Cluster left\n",
    "        cluster_0 = clusters.get(arr_0)\n",
    "        if cluster_0 is None:\n",
    "            new_sample.append(arr_0)\n",
    "        else:\n",
    "            cluster_num = min([cluster_num, cluster_0[\"cluster_num\"]])\n",
    "            left_level = max([level, cluster_0[\"level\"] + 1])\n",
    "            distance += cluster_0[\"total_dist\"]\n",
    "            count += cluster_0[\"count\"]\n",
    "        # Cluster right\n",
    "        cluster_1 = clusters.get(arr_1)\n",
    "        if cluster_1 is None:\n",
    "            new_sample.append(arr_1)\n",
    "        else:\n",
    "            cluster_num = min([cluster_num, cluster_1[\"cluster_num\"]])\n",
    "            right_level = max([level, cluster_1[\"level\"] + 1])\n",
    "            distance += cluster_1[\"total_dist\"]\n",
    "            count += cluster_1[\"count\"]\n",
    "\n",
    "        if cluster_0 and cluster_1:\n",
    "            if cluster_num == cluster_0[\"cluster_num\"]:\n",
    "                level = left_level\n",
    "            elif cluster_num == cluster_1[\"cluster_num\"]:\n",
    "                level = right_level\n",
    "        elif cluster_0:\n",
    "            level = left_level\n",
    "        elif cluster_1:\n",
    "            level = right_level\n",
    "\n",
    "        count += 1\n",
    "        distance += arr_i[2]\n",
    "\n",
    "        clusters[arr_i[-1]] = {\n",
    "            \"cluster_num\": cluster_num,\n",
    "            \"level\": level,\n",
    "            \"total_dist\": distance,\n",
    "            \"count\": count,\n",
    "            \"avg_dist\": distance / count,\n",
    "            \"samples_added\": new_sample,\n",
    "            \"sample_dist\": arr_i[2],\n",
    "        }\n",
    "\n",
    "        if cluster_num == max_clusters and i < len(arr) - 1:\n",
    "            max_clusters += 1\n",
    "\n",
    "        max_levels = max(max_levels, level)\n",
    "\n",
    "    return clusters, max_levels, max_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_clusters(clusters):\n",
    "    \"\"\"\n",
    "    Reorganize the clusters dictionary to be nested by cluster_num, then by level,\n",
    "    and include avg_dist, sample_dist, and samples within each level.\n",
    "\n",
    "    Parameters:\n",
    "    - clusters: A dictionary containing the original clusters information.\n",
    "\n",
    "    Returns:\n",
    "    - new_structure: A dictionary reorganized by cluster_num,\n",
    "                    then by level, with details.\n",
    "    \"\"\"\n",
    "    new_structure = {}\n",
    "    for _, info in clusters.items():\n",
    "        # Extract necessary information\n",
    "        cluster_num = info[\"cluster_num\"]\n",
    "        level = info[\"level\"]\n",
    "        samples = info.get(\"samples_added\", [])\n",
    "        # Initialize the structure if not present\n",
    "        if cluster_num not in new_structure:\n",
    "            new_structure[cluster_num] = {}\n",
    "\n",
    "        if level in new_structure[cluster_num]:\n",
    "            continue\n",
    "\n",
    "        if level == 1:\n",
    "            new_structure[cluster_num][level] = {\"samples\": []}\n",
    "        elif (level - 1) in new_structure[cluster_num]:\n",
    "            sam = deepcopy(new_structure[cluster_num][level - 1][\"samples\"])\n",
    "            new_structure[cluster_num][level] = {\"samples\": sam}\n",
    "\n",
    "        # Extending the samples list.\n",
    "        new_structure[cluster_num][level][\"samples\"].extend(samples)\n",
    "\n",
    "    return new_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_linkage(Z):\n",
    "    \"\"\"\n",
    "    Sort the linkage matrix Z in reverse order by distance and\n",
    "    then by cluster size (new_size).\n",
    "\n",
    "    Parameters:\n",
    "    - arr: linkage matrix\n",
    "\n",
    "    Returns:\n",
    "    - arr: Sorted linkage matrix\n",
    "    \"\"\"\n",
    "    # Adjusting linkage matrix to accommodate renumbering\n",
    "    arr = np.zeros((Z.shape[0], Z.shape[1] + 1))\n",
    "    arr[:, :-1] = Z.copy()\n",
    "    arr[:, -1] = np.arange(Z.shape[0] + 1, 2 * Z.shape[0] + 1)\n",
    "\n",
    "    # Sort by decreasing distance, then by increasing new_size\n",
    "    # arr = arr[arr[:, 2].argsort()[::-1]]\n",
    "    # arr = arr[arr[:, -2].argsort(kind=\"stable\")]\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "distance_matrix = pdist(data, metric=\"euclidean\")\n",
    "Z = linkage(distance_matrix, method=\"single\")\n",
    "\n",
    "# Sort the linkage matrix\n",
    "m_linkage = sort_linkage(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info, max_levels, max_clusters = create_clusters_opt(m_linkage)\n",
    "max_levels, max_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first row's last id is the cluster id that the linkage function uses in the rest of the linkage rows so columns 0 and 1. \n",
    "For example, \n",
    "the linkage function outputs 4 columns:\n",
    "1. sample/cluster id\n",
    "2. sample/cluster_id\n",
    "3. distance between merging\n",
    "4. total number of samples in the new cluster (it does not give the new cluster id, so I wrote a function to do that.\n",
    "\n",
    "Now with my function here is an example assuming 100 samples:\n",
    "row 0 - 23, 45, 1.78, 2, 100\n",
    "row 5 - 36, 100, 1.95, 3, 105\n",
    "\"\"\"\n",
    "m_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = reorganize_clusters(sample_info)\n",
    "clusters  # clusters : levels : samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate(link_arr, distance):\n",
    "    link_std = link_arr.std()\n",
    "    # print(\"stdev:\", link_std, \" | distance:\", distance)\n",
    "    if distance <= link_std / 1e3:\n",
    "        return \"exact duplicate\"\n",
    "    if distance <= link_std:\n",
    "        return \"near duplicate\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_outlier(level, distance, dist_arr):\n",
    "    for i, d in enumerate(dist_arr[level]):\n",
    "        if d == np.inf:\n",
    "            break\n",
    "    arr = dist_arr[level][:i]\n",
    "    # dist_mean, dist_std = dist_arr[level].mean(), dist_arr[level].std()\n",
    "    dist_mean, dist_std = arr.mean(), arr.std()\n",
    "    # print(\"dist stdev:\", dist_std * 2, \" | distance:\", abs(dist_mean - distance))\n",
    "    if abs(dist_mean - distance) > dist_std * 2:\n",
    "        return \"outlier\"\n",
    "    if level >= dist_arr.shape[0] * 2 / 3:\n",
    "        return \"potential outlier\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(\n",
    "    cluster: int,\n",
    "    level: int,\n",
    "    sample: int,\n",
    "    distance_array: np.ndarray,\n",
    "    distance_matrix: np.ndarray,\n",
    "    clusters: dict,\n",
    ") -> np.ndarray:\n",
    "    # Convert the condensed distance matrix to a square form\n",
    "    square_distance_matrix = squareform(distance_matrix)\n",
    "    print(\"Distance array:\", distance_array)\n",
    "\n",
    "    # clusters -> cluster_id : level : samples_added\n",
    "\n",
    "    for cluster_id, levels in clusters.items():\n",
    "        # Only compare samples on the same level, but different clusters\n",
    "        if cluster_id != cluster and level in levels:\n",
    "            new_samples = levels[level][\"samples\"]\n",
    "\n",
    "            # Get the minimum distance of all samples at each cluster\n",
    "            min_dist = distance_array[cluster_id]\n",
    "            # Should be optimized ->\n",
    "            # If all distances in an array instead of split into dict values,\n",
    "            # can take min over all instead of individually\n",
    "            for sample2 in new_samples:\n",
    "                sample_to_sample_dist = square_distance_matrix[sample, sample2]\n",
    "                # print(f\"Matrix [{sample},{sample2}] = {sample_to_sample_dist}\")\n",
    "                min_dist = min(min_dist, sample_to_sample_dist)\n",
    "            # print(f\"Min distance from cluster {cluster_id}: {min_dist}\")\n",
    "            # print()\n",
    "            distance_array[cluster_id] = min_dist\n",
    "\n",
    "    return distance_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tracking = {\n",
    "    i: {\n",
    "        \"cluster\": np.zeros(max_levels),\n",
    "        # \"distance\": np.full((max_levels, max_clusters), np.inf),\n",
    "        \"duplicate\": \"\",\n",
    "        \"outlier\": \"\",\n",
    "    }\n",
    "    for i in range(len(m_linkage) + 1)\n",
    "}\n",
    "\n",
    "cluster_matrix = np.full((max_levels, max_clusters), np.inf)\n",
    "# max levels: 51\n",
    "# max clusters: 24\n",
    "\n",
    "\"\"\"\n",
    "For each sample, compare distance for each sample to sample\n",
    "Ryan's code (with slight mods)\n",
    "\"\"\"\n",
    "print(f\"Max levels: {max_levels}\")\n",
    "print(f\"Max clusters: {max_clusters}\")\n",
    "# Merging the samples together by moving them up levels and clusters\n",
    "for sample_id, info in sample_info.items():\n",
    "    # Only computing for added samples\n",
    "    if not info[\"samples_added\"]:\n",
    "        continue\n",
    "\n",
    "    # The current sample info (origin info)\n",
    "    added_samples = info[\"samples_added\"]\n",
    "    current_cluster = info[\"cluster_num\"]\n",
    "    current_level = info[\"level\"]\n",
    "    current_dist = info[\"sample_dist\"]\n",
    "    is_duplicate = get_duplicate(\n",
    "        m_linkage[:, 2],\n",
    "        current_dist,\n",
    "    )\n",
    "\n",
    "    print(\"CURRENT DIST\", current_dist)\n",
    "    print(f\"Merging samples {added_samples} to sample {sample_id}\")\n",
    "    # Positions to \"move\" sample into\n",
    "    merge_level = current_level - 1\n",
    "    merge_cluster = current_cluster - 1\n",
    "\n",
    "    # Check each added sample, update info\n",
    "    for new_sample_id in added_samples:\n",
    "        # Get new samples info\n",
    "        new_sample_info = sample_tracking[new_sample_id]\n",
    "        print(f\"Level: {merge_level} | Cluster: {merge_cluster}\")\n",
    "\n",
    "        # As it moves up levels, track which cluster it was a part of at each level\n",
    "        # Set the new sample's cluster at current level to origin sample's cluster\n",
    "        new_sample_info[\"cluster\"][merge_level] = current_cluster\n",
    "        # print(new_sample_info[\"cluster\"])\n",
    "\n",
    "        # Set distance for new level and cluster to previous distance (to prevent distance compare becoming 0)\n",
    "        cluster_matrix[merge_level, merge_cluster] = current_dist\n",
    "\n",
    "        # At this level, get the distances from all samples\n",
    "        # print(\n",
    "        #     f\"Distance at [{merge_level},{merge_cluster}]: {new_sample_info['distance'][merge_level, merge_cluster]}\"\n",
    "        # )\n",
    "\n",
    "        # print(f\"Dist at level {merge_level}:\\n{dist_at_level}\")\n",
    "        updated_distance_at_level = get_distance(\n",
    "            cluster=current_cluster,\n",
    "            level=current_level,\n",
    "            sample=new_sample_id,\n",
    "            distance_array=cluster_matrix[merge_level],\n",
    "            distance_matrix=distance_matrix,\n",
    "            clusters=clusters,\n",
    "        )\n",
    "        cluster_matrix[merge_level] = updated_distance_at_level\n",
    "        outlier_status = get_outlier(\n",
    "            merge_level,\n",
    "            info[\"sample_dist\"],\n",
    "            cluster_matrix,\n",
    "        )\n",
    "        print(f\"Outlier: {outlier_status}\")\n",
    "        new_sample_info[\"outlier\"] = outlier_status\n",
    "    print(\"\\n>>>>>   <<<<<\\n\")\n",
    "\n",
    "# for s in sample_tracking.values():\n",
    "#     print(s[\"cluster\"])\n",
    "# print(sample_tracking[0])\n",
    "print(cluster_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refactored to use 1 cluster matrix, and a clusters per level dictionary\n",
    "\"\"\"\n",
    "cluster_matrix = np.full((max_levels, max_clusters), np.inf)\n",
    "square_distance_matrix = squareform(distance_matrix)\n",
    "\n",
    "print(cluster_matrix.shape)\n",
    "print(square_distance_matrix.shape)\n",
    "\n",
    "level_clusters = {}\n",
    "for cluster, level_info in clusters.items():\n",
    "    for lvl, samples_dict in level_info.items():\n",
    "        if lvl not in level_clusters:\n",
    "            level_clusters[lvl] = []\n",
    "        level_clusters[lvl].append(samples_dict[\"samples\"])\n",
    "\"\"\"\n",
    "key: level\n",
    "value: list of lists\n",
    "    -> list of clusters where clusters are lists of samples in that cluster\n",
    "ex.\n",
    "level 1:\n",
    "              cluster_0               cluster_1\n",
    "    [[sample1, sample2, sample3], [sample4, sample5]]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_sample_avg_dist(sample_id, sample_info):\n",
    "    \"\"\"\n",
    "    Queries the total sample info dictionary to find\n",
    "    where the sample is first added to the clusters\n",
    "    Uses the average distance at that cluster\n",
    "    \"\"\"\n",
    "    # print(\"SAMPLE:\", sample_id)\n",
    "    for _, info in sample_info.items():\n",
    "        samples = info[\"samples_added\"]\n",
    "        # print(samples)\n",
    "        if sample_id in samples:\n",
    "            return info[\"avg_dist\"]\n",
    "    else:\n",
    "        raise IndexError(\"Sample not found in list\")\n",
    "\n",
    "\n",
    "# Get distance from each cluster to all others\n",
    "# Take minimum distance of samples in a cluster\n",
    "for level, clusters_of_samples in level_clusters.items():\n",
    "    level = level - 1\n",
    "    print(f\"\\t\\tLevel {level}\")\n",
    "    if len(clusters_of_samples) <= 1:\n",
    "        cluster_matrix[level, 0] = get_sample_avg_dist(\n",
    "            clusters_of_samples[0][0], sample_info\n",
    "        )\n",
    "    for cid1, samples1 in enumerate(clusters_of_samples):\n",
    "        print(\"\\tMain cluster:\", cid1)\n",
    "        for cid2, samples2 in enumerate(clusters_of_samples[cid1 + 1 :]):\n",
    "            cid2 = cid1 + cid2 + 1\n",
    "            print(\"Second cluster:\", cid2)\n",
    "            print(samples1, samples2)\n",
    "\n",
    "            for sample1 in samples1:\n",
    "                for sample2 in samples2:\n",
    "                    if sample1 == sample2:\n",
    "                        continue\n",
    "                    v = square_distance_matrix[sample1, sample2]\n",
    "                    print(sample1, sample2, \"=>\", v)\n",
    "                    # Because it is square, fill both (i,j) and (j,i) with minimum\n",
    "                    cluster_matrix[level, cid1] = min(cluster_matrix[level, cid1], v)\n",
    "                    cluster_matrix[level, cid2] = min(cluster_matrix[level, cid2], v)\n",
    "\n",
    "sample_tracking = {\n",
    "    i: {\n",
    "        \"cluster\": np.zeros(max_levels),\n",
    "        \"duplicate\": \"\",\n",
    "        \"outlier\": \"\",\n",
    "    }\n",
    "    for i in range(len(m_linkage) + 1)\n",
    "}\n",
    "\n",
    "for cluster_id, info in sample_info.items():\n",
    "    added_samples = info.get(\"samples_added\")\n",
    "    if not added_samples:\n",
    "        continue\n",
    "\n",
    "    added_samples = info[\"samples_added\"]\n",
    "    current_cluster = info[\"cluster_num\"]\n",
    "    current_level = info[\"level\"]\n",
    "    current_dist = info[\"sample_dist\"]\n",
    "    is_duplicate = get_duplicate(\n",
    "        m_linkage[:, 2],\n",
    "        current_dist,\n",
    "    )\n",
    "\n",
    "    merge_level = current_level - 1\n",
    "    merge_cluster = current_cluster - 1\n",
    "\n",
    "    for sample_id in added_samples:\n",
    "        sample_info_to_update = sample_tracking[sample_id]\n",
    "        sample_info_to_update[\"cluster\"][merge_level] = current_cluster\n",
    "\n",
    "        outlier_status = get_outlier(\n",
    "            merge_level,\n",
    "            current_dist,\n",
    "            cluster_matrix,\n",
    "        )\n",
    "        sample_info_to_update[\"outlier\"] = outlier_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where does it enter a cluster -> potential outlier\n",
    "\n",
    "never enters, how far from NN compared to how far clusters tend to be from other clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WIP\"\"\"\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cluster_id,\n",
    "        cluster_num,\n",
    "        level,\n",
    "        total_dist,\n",
    "        count,\n",
    "        avg_dist,\n",
    "        samples_added,\n",
    "        sample_dist,\n",
    "    ):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.cluster_num = cluster_num\n",
    "        self.level = level\n",
    "        self.total_dist = total_dist\n",
    "        self.count = count\n",
    "        self.avg_dist = avg_dist\n",
    "        self.samples_added = samples_added\n",
    "        self.sample_dist = sample_dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
