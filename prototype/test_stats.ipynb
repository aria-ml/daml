{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from daml._internal.metrics.stats import DatasetStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MNIST Data\n",
    "import hashlib\n",
    "import os\n",
    "import typing\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def download_mnist() -> str:\n",
    "    \"\"\"Code to download mnist originates from keras/datasets:\n",
    "\n",
    "    https://github.com/keras-team/keras/blob/v2.15.0/keras/datasets/mnist.py#L25-L86\n",
    "    \"\"\"\n",
    "    origin_folder = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n",
    "    path = _get_file(\n",
    "        \"mnist.npz\",\n",
    "        origin=origin_folder + \"mnist.npz\",\n",
    "        file_hash=(\"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"),\n",
    "    )\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def _get_file(\n",
    "    fname: str,\n",
    "    origin: str,\n",
    "    file_hash: typing.Optional[str] = None,\n",
    "):\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".keras\")\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join(\"/tmp\", \".keras\")\n",
    "    datadir = os.path.join(datadir_base, \"datasets\")\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fname = os.fspath(fname) if isinstance(fname, os.PathLike) else fname\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "        if file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        try:\n",
    "            error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg)) from e\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason)) from e\n",
    "        except (Exception, KeyboardInterrupt):\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "        if os.path.exists(fpath) and file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            raise ValueError(\n",
    "                \"Incomplete or corrupted file detected. \"\n",
    "                f\"The sha256 file hash does not match the provided value \"\n",
    "                f\"of {file_hash}.\",\n",
    "            )\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def _validate_file(fpath, file_hash, chunk_size=65535):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(fpath, \"rb\") as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return str(hasher.hexdigest()) == str(file_hash)\n",
    "\n",
    "\n",
    "mnist_path = download_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "rng = np.random.default_rng(33)\n",
    "size = 10000\n",
    "\n",
    "with np.load(mnist_path, allow_pickle=True) as fp:\n",
    "    test_images, labels = fp[\"x_train\"][:size], fp[\"y_train\"][:size]\n",
    "\n",
    "norm_test_imgs = np.repeat(test_images[:, np.newaxis, :, :], 3, axis=1) / 255\n",
    "jitter = rng.integers(10, size=norm_test_imgs.shape)\n",
    "norm_test_imgs += jitter\n",
    "\n",
    "\n",
    "# rng.shuffle(test_images)\n",
    "# rng.shuffle(norm_test_imgs)\n",
    "\n",
    "print(test_images.shape)\n",
    "print(norm_test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_stats = DatasetStats(test_images)\n",
    "dataset_multistats = DatasetStats(test_images[:100])\n",
    "# imagestats = SingleImageStats(norm_test_imgs[0])\n",
    "# image_stats = ImageStats(test_images[0])\n",
    "# image_stats.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_stats.image_stats[0].__dict__\n",
    "dataset_multistats.ch_percentiles\n",
    "mask = dataset_multistats.get_channel_mask(3, 2)\n",
    "dataset_multistats.ch_map[mask][:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import linter\n",
    "\n",
    "import daml._internal.metrics.hash as hasher\n",
    "import daml._internal.metrics.stats as stats\n",
    "\n",
    "reload(stats)\n",
    "reload(hasher)\n",
    "reload(linter)\n",
    "\n",
    "count = 5000\n",
    "lint = linter.Linter(norm_test_imgs[:count])\n",
    "results = lint.get_outliers(\"modzscore\", 3.75)\n",
    "print(f\"{len(results)} ({round(100*len(results)/count,2)}%) outliers found.\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = lint.get_duplicates()\n",
    "print(f\"{len(dupes['exact'])} ({round(100*len(dupes['exact'])/count,2)}%) exact duplicates found.\")\n",
    "print(f\"{len(dupes['near'])} ({round(100*len(dupes['near'])/count,2)}%) near duplicates found.\")\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.fromarray(test_images[23]).show()\n",
    "Image.fromarray(test_images[4383]).show()\n",
    "Image.fromarray(test_images[80]).show()\n",
    "Image.fromarray(test_images[2448]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
