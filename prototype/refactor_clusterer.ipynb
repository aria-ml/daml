{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Clustering Code 5/10/24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the code for the clusterer - adjusting it here first for testing.\n",
    "Need to adjust such that each function really only depends upon 1 function above it\n",
    "Also split up functions as much as possible - want small bits of code\n",
    "\"\"\"\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets as dsets\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condensed_distance_array(data):\n",
    "    return pdist(data, metric=\"euclidean\")\n",
    "\n",
    "\n",
    "def get_square_distance_matrix(condensed_distance_array):\n",
    "    return squareform(condensed_distance_array)\n",
    "\n",
    "\n",
    "def get_linkage_arr(condensed_distance_array):\n",
    "    return linkage(condensed_distance_array, method=\"single\")\n",
    "\n",
    "\n",
    "def extend_linkage(link_arr):\n",
    "    \"\"\"\n",
    "    Adds a column to the linkage matrix Z that tracks the new id assigned\n",
    "    to each row\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z\n",
    "        linkage matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arr\n",
    "        linkage matrix with adjusted shape, new shape (Z.shape[0], Z.shape[1]+1)\n",
    "    \"\"\"\n",
    "    # Adjusting linkage matrix to accommodate renumbering\n",
    "    rows, cols = link_arr.shape\n",
    "    arr = np.zeros((rows, cols + 1))\n",
    "    arr[:, :-1] = link_arr\n",
    "    arr[:, -1] = np.arange(rows + 1, 2 * rows + 1)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def get_extended_linkage(condensed_distance_array):\n",
    "    link_arr = get_linkage_arr(condensed_distance_array)\n",
    "    return extend_linkage(link_arr)\n",
    "\n",
    "\n",
    "def fill_missing_cluster_level(left_id, right_id, level, clusters):\n",
    "    if left_id:\n",
    "        left_level = left_id[0] + 1\n",
    "        left_cluster = left_id[1]\n",
    "        if level != left_level:\n",
    "            for level_id in range(level - 1, left_level - 2, -1):\n",
    "                if left_cluster not in clusters[level_id]:\n",
    "                    clusters[level_id][left_cluster] = {\n",
    "                        \"cluster_merged\": False,\n",
    "                        \"count\": clusters[left_level - 1][left_cluster][\"count\"],\n",
    "                        \"avg_dist\": clusters[left_level - 1][left_cluster][\"avg_dist\"],\n",
    "                        \"dist_std\": clusters[left_level - 1][left_cluster][\"dist_std\"],\n",
    "                        \"samples\": clusters[left_level - 1][left_cluster][\"samples\"],\n",
    "                        \"sample_dist\": clusters[left_level - 1][left_cluster][\"sample_dist\"],\n",
    "                        \"outside_1-std\": False,\n",
    "                        \"outside_2-std\": False,\n",
    "                    }\n",
    "    if right_id:\n",
    "        right_level = right_id[0] + 1\n",
    "        right_cluster = right_id[1]\n",
    "        if level != right_level:\n",
    "            for level_id in range(level - 1, right_level - 2, -1):\n",
    "                if right_cluster not in clusters[level_id]:\n",
    "                    clusters[level_id][right_cluster] = {\n",
    "                        \"cluster_merged\": False,\n",
    "                        \"count\": clusters[right_level - 1][right_cluster][\"count\"],\n",
    "                        \"avg_dist\": clusters[right_level - 1][right_cluster][\"avg_dist\"],\n",
    "                        \"dist_std\": clusters[right_level - 1][right_cluster][\"dist_std\"],\n",
    "                        \"samples\": clusters[right_level - 1][right_cluster][\"samples\"],\n",
    "                        \"sample_dist\": clusters[right_level - 1][right_cluster][\"sample_dist\"],\n",
    "                        \"outside_1-std\": False,\n",
    "                        \"outside_2-std\": False,\n",
    "                    }\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterer:\n",
    "    def __init__(self, dataset: np.ndarray, min_num_samples_per_cluster: Optional[int] = None):\n",
    "        self.min_cluster_size: Optional[int] = min_num_samples_per_cluster\n",
    "        self._on_init(dataset)\n",
    "\n",
    "    def _on_init(self, x):\n",
    "        self._data: np.ndarray = x\n",
    "        self.num_samples = len(x)\n",
    "        self.darr: np.ndarray = get_condensed_distance_array(x)\n",
    "        self.sqdmat: np.ndarray = get_square_distance_matrix(self.darr)\n",
    "        self.larr: np.ndarray = get_extended_linkage(self.darr)\n",
    "        self.max_clusters: int = np.count_nonzero(self.larr[:, 3] == 2)\n",
    "        self.last_merge_level: int = 1\n",
    "        self.min_num_samples_per_cluster: int = (\n",
    "            int(min(100, max(2, self.num_samples * 0.05))) if not self.min_cluster_size else self.min_cluster_size\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, x: np.ndarray):\n",
    "        self._on_init(x)\n",
    "\n",
    "    def create_clusters(self) -> Dict[int, Any]:\n",
    "        \"\"\"Generates clusters based on linkage matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, Any]\n",
    "            Cluster information\n",
    "        \"\"\"\n",
    "        cluster_num = 0\n",
    "        cluster_tracking = 0\n",
    "        clusters = {}  # Dictionary to store clusters\n",
    "        tracking = {}  # Dictionary to associate new cluster ids with actual clusters\n",
    "\n",
    "        # Walking through the linkage array to generate clusters\n",
    "        for arr_i in self.larr:\n",
    "            level = 0\n",
    "\n",
    "            left_count = 0\n",
    "            right_count = 0\n",
    "            merged = False\n",
    "\n",
    "            arr_0 = int(arr_i[0])  # Grabbing the left id\n",
    "            arr_1 = int(arr_i[1])  # Grabbing the right id\n",
    "            dist = arr_i[2]  # Getting the distance between the left and right ids\n",
    "\n",
    "            new_sample = []\n",
    "            sample_dist = np.array([dist], dtype=np.float16)\n",
    "\n",
    "            # Linkage matrix first column id\n",
    "            left_id = tracking.get(arr_0)  # Determining if the id is already associated with a cluster\n",
    "            if left_id is None:\n",
    "                new_sample.append(arr_0)\n",
    "            else:\n",
    "                left_cluster = left_id[1]\n",
    "                left_level = left_id[0] + 1\n",
    "                left_count = clusters[left_id[0]][left_cluster][\"count\"]\n",
    "                left_sample = clusters[left_id[0]][left_cluster][\"samples\"]\n",
    "                sample_dist = np.concatenate([clusters[left_id[0]][left_cluster][\"sample_dist\"], sample_dist])\n",
    "\n",
    "            # Linkage matrix second column id\n",
    "            right_id = tracking.get(arr_1)  # Determining if the id is already associated with a cluster\n",
    "            if right_id is None:\n",
    "                new_sample.append(arr_1)\n",
    "            else:\n",
    "                right_cluster = right_id[1]\n",
    "                right_level = right_id[0] + 1\n",
    "                right_count = clusters[right_id[0]][right_cluster][\"count\"]\n",
    "                right_sample = clusters[right_id[0]][right_cluster][\"samples\"]\n",
    "                sample_dist = np.concatenate([clusters[right_id[0]][right_cluster][\"sample_dist\"], sample_dist])\n",
    "\n",
    "            # Aggregate samples, determine cluster number, and get the level\n",
    "            if left_id and right_id:\n",
    "                if left_count > right_count:\n",
    "                    samples = np.concatenate([left_sample, right_sample])\n",
    "                else:\n",
    "                    samples = np.concatenate([right_sample, left_sample])\n",
    "                cluster_num = min([left_cluster, right_cluster])\n",
    "                merged = max([left_cluster, right_cluster])\n",
    "                level = max([left_level, right_level])\n",
    "            elif left_id:\n",
    "                samples = np.concatenate([left_sample, new_sample])\n",
    "                cluster_num = left_cluster\n",
    "                level = left_level\n",
    "            elif right_id:\n",
    "                samples = np.concatenate([right_sample, new_sample])\n",
    "                cluster_num = right_cluster\n",
    "                level = right_level\n",
    "            else:\n",
    "                samples = np.array(new_sample, dtype=np.int32)\n",
    "                cluster_num = cluster_tracking\n",
    "\n",
    "            dist_avg = np.mean(sample_dist)\n",
    "            dist_std = np.std(sample_dist) if sample_dist.shape[0] > 1 else 1e-5\n",
    "\n",
    "            out1 = dist_avg + dist_std\n",
    "            out2 = out1 + dist_std\n",
    "\n",
    "            # Initialize the structure if not present\n",
    "            if level not in clusters:\n",
    "                clusters[level] = {\n",
    "                    cluster_num: {\n",
    "                        \"cluster_merged\": merged,\n",
    "                        \"count\": samples.shape[0],\n",
    "                        \"avg_dist\": dist_avg,\n",
    "                        \"dist_std\": dist_std,\n",
    "                        \"samples\": samples,\n",
    "                        \"sample_dist\": sample_dist,\n",
    "                        \"outside_1-std\": dist > out1,\n",
    "                        \"outside_2-std\": dist > out2,\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                clusters[level][cluster_num] = {\n",
    "                    \"cluster_merged\": merged,\n",
    "                    \"count\": samples.shape[0],\n",
    "                    \"avg_dist\": dist_avg,\n",
    "                    \"dist_std\": dist_std,\n",
    "                    \"samples\": samples,\n",
    "                    \"sample_dist\": sample_dist,\n",
    "                    \"outside_1-std\": dist > out1,\n",
    "                    \"outside_2-std\": dist > out2,\n",
    "                }\n",
    "\n",
    "            tracking[int(arr_i[-1])] = (level, cluster_num)  # Associates the new linkage id with the correct cluster\n",
    "\n",
    "            if not left_id and not right_id:\n",
    "                # Making sure that new clusters get unique numbers\n",
    "                cluster_tracking += 1\n",
    "            else:\n",
    "                # Fill missing cluster levels for continuity.\n",
    "                # Ensures all levels have consistent information across cluster changes.\n",
    "                clusters = fill_missing_cluster_level(left_id, right_id, level, clusters)\n",
    "\n",
    "            # Only tracking the levels in which clusters merge for the cluster distance matrix\n",
    "            if merged:\n",
    "                self.last_merge_level = max(self.last_merge_level, level + 1)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def get_cluster_distances(self, clusters):\n",
    "        # this is the cluster distance matrix\n",
    "        cluster_matrix = np.full((self.last_merge_level, self.max_clusters, self.max_clusters), -1.0, dtype=np.float32)\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            if level < self.last_merge_level:\n",
    "                cluster_ids = sorted(cluster_set.keys())\n",
    "                for i, cluster_id in enumerate(cluster_ids):\n",
    "                    cluster_matrix[level, cluster_id, cluster_id] = clusters[level][cluster_id][\"avg_dist\"]\n",
    "                    for int_id in range(i + 1, len(cluster_ids)):\n",
    "                        compare_id = cluster_ids[int_id]\n",
    "                        sample_a = clusters[level][cluster_id][\"samples\"]\n",
    "                        sample_b = clusters[level][compare_id][\"samples\"]\n",
    "                        min_mat = self.sqdmat[np.ix_(sample_a, sample_b)].min()\n",
    "                        cluster_matrix[level, cluster_id, compare_id] = min_mat\n",
    "                        cluster_matrix[level, compare_id, cluster_id] = min_mat\n",
    "\n",
    "        return cluster_matrix\n",
    "\n",
    "    def get_merge_levels(self, clusters):\n",
    "        \"\"\"\n",
    "        Runs through the clusters dictionary determining when clusters merge,\n",
    "        and how close are those clusters when they merge.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clusters:\n",
    "            A dictionary containing the original clusters information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        merge_clusters:\n",
    "            A dictionary with each clusters merge history\n",
    "        \"\"\"\n",
    "\n",
    "        merge_clusters = {\"merge\": {}, \"likely_merge\": {}, \"no_merge\": {}}\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            cluster_ids = sorted(cluster_set.keys())\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                # Extract necessary information\n",
    "                samples = clusters[level][cluster_id][\"samples\"]\n",
    "                merged = clusters[level][cluster_id][\"cluster_merged\"]\n",
    "                out1 = clusters[level][cluster_id][\"outside_1-std\"]\n",
    "                out2 = clusters[level][cluster_id][\"outside_2-std\"]\n",
    "\n",
    "                if merged:\n",
    "                    if out2:\n",
    "                        if len(samples) < self.min_num_samples_per_cluster:\n",
    "                            if cluster_id not in merge_clusters[\"likely_merge\"]:\n",
    "                                merge_clusters[\"likely_merge\"][cluster_id] = {level: [merged, \"low\"]}\n",
    "                            if level not in merge_clusters[\"likely_merge\"][cluster_id]:\n",
    "                                merge_clusters[\"likely_merge\"][cluster_id][level] = [merged, \"low\"]\n",
    "                        else:\n",
    "                            if cluster_id not in merge_clusters[\"no_merge\"]:\n",
    "                                merge_clusters[\"no_merge\"][cluster_id] = {level: [merged]}\n",
    "                            if level not in merge_clusters[\"no_merge\"][cluster_id]:\n",
    "                                merge_clusters[\"no_merge\"][cluster_id][level] = [merged]\n",
    "\n",
    "                    elif out1 and len(samples) >= self.min_num_samples_per_cluster:\n",
    "                        if cluster_id not in merge_clusters[\"likely_merge\"]:\n",
    "                            merge_clusters[\"likely_merge\"][cluster_id] = {level: [merged]}\n",
    "                        if level not in merge_clusters[\"likely_merge\"][cluster_id]:\n",
    "                            merge_clusters[\"likely_merge\"][cluster_id][level] = [merged]\n",
    "\n",
    "                    else:\n",
    "                        if cluster_id not in merge_clusters[\"merge\"]:\n",
    "                            merge_clusters[\"merge\"][cluster_id] = {level: [merged]}\n",
    "                        if level not in merge_clusters[\"merge\"][cluster_id]:\n",
    "                            merge_clusters[\"merge\"][cluster_id][level] = [merged]\n",
    "\n",
    "        return merge_clusters\n",
    "\n",
    "    def generate_merge_list(self, cluster_merges, cluster_matrix):\n",
    "        merge_list, merge_mean, intra_max, additional_check = self.cluster_merging(cluster_merges, cluster_matrix)\n",
    "        desired_merge, merge = self.get_desired_merge(merge_mean, intra_max, additional_check)\n",
    "\n",
    "        j = 0\n",
    "        for i, select in enumerate(desired_merge):\n",
    "            if select:\n",
    "                merge_list[i].append(\"merge\")\n",
    "            else:\n",
    "                if merge[j]:\n",
    "                    merge_list[i].append(\"merge\")\n",
    "                else:\n",
    "                    merge_list[i].append(\"no-merge\")\n",
    "                j += 1\n",
    "\n",
    "        merge_list = sorted(merge_list, reverse=True)\n",
    "        return merge_list\n",
    "\n",
    "    def cluster_merging(self, cluster_merges, cluster_matrix):\n",
    "        intra_max = []\n",
    "        merge_mean = []\n",
    "        merge_list = []\n",
    "        additional_check = []\n",
    "        # Process each merge type\n",
    "        for merge_type, merge_clusters in cluster_merges.items():\n",
    "            for outer_cluster, inner_clusters in merge_clusters.items():\n",
    "                for level, cluster_list in inner_clusters.items():\n",
    "                    # Determine if there is a small far merge\n",
    "                    if len(cluster_list) == 2:\n",
    "                        additional_check.append(True)\n",
    "                    else:\n",
    "                        additional_check.append(False)\n",
    "                    inner_cluster = cluster_list[0]\n",
    "\n",
    "                    # Get the slice of the distance matrix up to the level before merging\n",
    "                    distances = cluster_matrix[:level, outer_cluster, inner_cluster]\n",
    "                    # print(f\"Negative check for {outer_cluster}-{inner_cluster} at {level} : {np.any(distances<0)}\")\n",
    "                    # if np.any(distances < 0):\n",
    "                    #     print(distances)\n",
    "                    intra_distance = cluster_matrix[:, outer_cluster, outer_cluster]\n",
    "                    mask = intra_distance >= 0\n",
    "                    intra_filtered = intra_distance[mask]\n",
    "                    intra_max.append(np.max(intra_filtered))\n",
    "\n",
    "                    # Grabbing the corresponding desired values\n",
    "                    if merge_type == \"merge\":\n",
    "                        merge_mean.append(np.max(distances))\n",
    "                    else:\n",
    "                        merge_mean.append(np.mean(distances))\n",
    "\n",
    "                    merge_list.append([level, outer_cluster, inner_cluster])\n",
    "\n",
    "        return merge_list, merge_mean, intra_max, additional_check\n",
    "\n",
    "    def get_desired_merge(self, merge_mean, intra_max, additional_check):\n",
    "        intra_max = np.unique(intra_max)\n",
    "        intra_value = np.log(intra_max)\n",
    "        intra_value = intra_value.mean() + 2 * intra_value.std()\n",
    "        merge_value = np.log(merge_mean)\n",
    "        desired_merge = merge_value < intra_value\n",
    "\n",
    "        check = merge_value[~desired_merge]\n",
    "        check = np.abs((check - intra_value) / intra_value)\n",
    "        mask = check < 1\n",
    "        good = check[mask].mean() + check[mask].std()\n",
    "        merge = check < good\n",
    "        return desired_merge, merge\n",
    "\n",
    "    def get_last_merge_levels(self, merge_list):\n",
    "        last_good_merge_levels = {}\n",
    "        for entry in merge_list:\n",
    "            level, outer_cluster, inner_cluster, status = entry\n",
    "            if status == \"no-merge\":\n",
    "                if outer_cluster not in last_good_merge_levels:\n",
    "                    last_good_merge_levels[outer_cluster] = 1\n",
    "                if inner_cluster not in last_good_merge_levels:\n",
    "                    last_good_merge_levels[inner_cluster] = 1\n",
    "                if last_good_merge_levels[outer_cluster] > level:\n",
    "                    last_good_merge_levels[outer_cluster] = level - 1\n",
    "            else:\n",
    "                if outer_cluster in last_good_merge_levels:\n",
    "                    last_good_merge_levels[outer_cluster] = max(last_good_merge_levels[outer_cluster], level)\n",
    "        return last_good_merge_levels\n",
    "\n",
    "    def fill_dedup_std_list(self, last_merge_levels):\n",
    "        pass\n",
    "\n",
    "    def find_duplicates(self, dedup_std_list):\n",
    "        diag_mask = np.ones(self.sqdmat.shape, dtype=bool)\n",
    "        np.fill_diagonal(diag_mask, 0)\n",
    "        diag_mask = np.triu(diag_mask)\n",
    "\n",
    "        exact_mask = self.sqdmat < (np.mean(dedup_std_list) / 100)\n",
    "        exact_indices = np.nonzero(exact_mask & diag_mask)\n",
    "        exact_dedup = list(zip(exact_indices[0], exact_indices[1]))\n",
    "\n",
    "        possible_mask = self.sqdmat < np.mean(dedup_std_list)\n",
    "        possible_indices = np.nonzero(possible_mask & diag_mask & ~exact_mask)\n",
    "        possible_dedup = list(zip(possible_indices[0], possible_indices[1]))\n",
    "\n",
    "        return exact_dedup, possible_dedup\n",
    "\n",
    "    def find_outliers(self, clusters, last_merge_levels):\n",
    "        \"\"\"\n",
    "        The clusters dictionary contains whether the added sample/cluster\n",
    "        was outside 1 standard deviation or outside 2 standard deviations.\n",
    "        last_merge_levels contains the last good merge for each cluster we care about\n",
    "        Using this information to determine when the sample was added to the cluster\n",
    "        and how far it was from the cluster when it was added\n",
    "\n",
    "        \"\"\"\n",
    "        outliers = []\n",
    "        possible_outliers = []\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            cluster_ids = sorted(cluster_set.keys())\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                # Extract necessary information\n",
    "                samples = clusters[level][cluster_id][\"samples\"]\n",
    "                merged = clusters[level][cluster_id][\"cluster_merged\"]\n",
    "                out1 = clusters[level][cluster_id][\"outside_1-std\"]\n",
    "                out2 = clusters[level][cluster_id][\"outside_2-std\"]\n",
    "\n",
    "                if cluster_id in last_merge_levels and not merged:\n",
    "                    if level > last_merge_levels[cluster_id] and out2:\n",
    "                        outliers.append(samples[-1])\n",
    "                    elif (\n",
    "                        level > last_merge_levels[cluster_id]\n",
    "                        and out1\n",
    "                        and len(samples) >= self.min_num_samples_per_cluster\n",
    "                    ):\n",
    "                        possible_outliers.append(samples[-1])\n",
    "        return outliers, possible_outliers\n",
    "\n",
    "    def run(self):\n",
    "        sample_info = self.create_clusters()\n",
    "\n",
    "        print(\"Clusters per level:\")\n",
    "        for level, stuff in sample_info.items():\n",
    "            print(f\"\\t{level}\")\n",
    "            for cluster, more_stuff in stuff.items():\n",
    "                print(f\"\\t\\t{cluster} - {more_stuff['cluster_merged']}\")\n",
    "\n",
    "        if self.max_clusters > 1:\n",
    "            cluster_matrix = self.get_cluster_distances(sample_info)\n",
    "            merge_levels = self.get_merge_levels(sample_info)\n",
    "            merge_list = self.generate_merge_list(merge_levels, cluster_matrix)\n",
    "            last_merge_levels = self.get_last_merge_levels(merge_list)\n",
    "        else:\n",
    "            last_merge_levels = {0: int(max(self.num_samples * 0.1, self.min_num_samples_per_cluster))}\n",
    "\n",
    "        outliers, potential_outliers = self.find_outliers(sample_info, last_merge_levels)\n",
    "\n",
    "        dedup_std = []\n",
    "        for cluster, level in last_merge_levels.items():\n",
    "            level_cluster = sample_info[level][cluster]\n",
    "            samples = level_cluster[\"samples\"]\n",
    "            if samples.shape[0] < self.min_num_samples_per_cluster:\n",
    "                outliers.extend(samples.tolist())\n",
    "            else:\n",
    "                dedup_std.append(level_cluster[\"dist_std\"])\n",
    "\n",
    "        duplicates, near_duplicates = self.find_duplicates(dedup_std)\n",
    "\n",
    "        ret = {\n",
    "            \"outliers\": outliers,\n",
    "            \"potential_outliers\": potential_outliers,\n",
    "            \"duplicates\": duplicates,\n",
    "            \"near_duplicates\": near_duplicates,\n",
    "        }\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterer2:\n",
    "    def __init__(self, dataset: np.ndarray):\n",
    "        \"\"\" \"\"\"\n",
    "        # This is done to update the state rather than instantiate a new class when new data is passed in\n",
    "        self._on_init(dataset)\n",
    "\n",
    "    def _on_init(self, dataset: np.ndarray):\n",
    "        self._data: np.ndarray = dataset\n",
    "        self.num_samples = len(dataset)\n",
    "        self.darr: np.ndarray = get_condensed_distance_array(dataset)\n",
    "        self.sqdmat: np.ndarray = get_square_distance_matrix(self.darr)\n",
    "        self.larr: np.ndarray = get_extended_linkage(self.darr)\n",
    "        self.max_clusters: int = np.count_nonzero(self.larr[:, 3] == 2)\n",
    "        self.last_merge_level: int = 1\n",
    "\n",
    "        min_num = int(self.num_samples * 0.05)\n",
    "        self.min_num_samples_per_cluster = clamp(min_num, 2, 100)\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, x: np.ndarray):\n",
    "        self._on_init(x)\n",
    "\n",
    "    def create_clusters(self) -> Dict[int, Any]:\n",
    "        \"\"\"Generates clusters based on linkage matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, Any]\n",
    "            Cluster information\n",
    "        \"\"\"\n",
    "        cluster_num = 0\n",
    "        cluster_tracking = 0\n",
    "        clusters = {}  # Dictionary to store clusters\n",
    "        tracking = {}  # Dictionary to associate new cluster ids with actual clusters\n",
    "\n",
    "        # Walking through the linkage array to generate clusters\n",
    "        for arr_i in self.larr:\n",
    "            level = 0\n",
    "            merged = False\n",
    "\n",
    "            arr_0 = int(arr_i[0])  # Grabbing the left id\n",
    "            arr_1 = int(arr_i[1])  # Grabbing the right id\n",
    "            dist = arr_i[2]\n",
    "            sample_dist = np.array([dist], dtype=np.float16)\n",
    "\n",
    "            # Linkage matrix first column id\n",
    "            left_id = tracking.get(arr_0)  # Determining if the id is already associated with a cluster\n",
    "            if left_id:\n",
    "                left_cluster = left_id[1]\n",
    "                left_level = left_id[0] + 1\n",
    "                left_sample = clusters[left_id[0]][left_cluster][\"samples\"]\n",
    "                sample_dist = np.concatenate([clusters[left_id[0]][left_cluster][\"sample_dist\"], sample_dist])\n",
    "            # Linkage matrix second column id\n",
    "            right_id = tracking.get(arr_1)  # Determining if the id is already associated with a cluster\n",
    "            if right_id:\n",
    "                right_cluster = right_id[1]\n",
    "                right_level = right_id[0] + 1\n",
    "                right_sample = clusters[right_id[0]][right_cluster][\"samples\"]\n",
    "                sample_dist = np.concatenate([clusters[right_id[0]][right_cluster][\"sample_dist\"], sample_dist])\n",
    "\n",
    "            # Aggregate samples, determine cluster number, and get the level\n",
    "            if left_id and right_id:\n",
    "                if clusters[left_id[0]][left_cluster][\"count\"] > clusters[right_id[0]][right_cluster][\"count\"]:\n",
    "                    samples = np.concatenate([left_sample, right_sample])\n",
    "                else:\n",
    "                    samples = np.concatenate([right_sample, left_sample])\n",
    "                cluster_num = min([left_cluster, right_cluster])\n",
    "                merged = max([left_cluster, right_cluster])\n",
    "                level = max([left_level, right_level])\n",
    "                # Only tracking the levels in which clusters merge for the cluster distance matrix\n",
    "                self.last_merge_level = max(self.last_merge_level, level + 1)\n",
    "            elif left_id:\n",
    "                samples = np.concatenate([left_sample, [arr_1]])\n",
    "                cluster_num = left_cluster\n",
    "                level = left_level\n",
    "            elif right_id:\n",
    "                samples = np.concatenate([right_sample, [arr_0]])\n",
    "                cluster_num = right_cluster\n",
    "                level = right_level\n",
    "            else:\n",
    "                samples = np.array([arr_0, arr_1], dtype=np.int32)\n",
    "                cluster_num = cluster_tracking\n",
    "\n",
    "            # Calculate distances\n",
    "            dist_avg = np.mean(sample_dist)\n",
    "            dist_std = np.std(sample_dist) if sample_dist.shape[0] > 1 else 1e-5\n",
    "\n",
    "            out1 = dist_avg + dist_std\n",
    "            out2 = out1 + dist_std\n",
    "\n",
    "            # Initialize the structure if not present\n",
    "            if level not in clusters:\n",
    "                clusters[level] = {}\n",
    "\n",
    "            clusters[level][cluster_num] = {\n",
    "                \"cluster_merged\": merged,\n",
    "                \"count\": samples.shape[0],\n",
    "                \"avg_dist\": dist_avg,\n",
    "                \"dist_std\": dist_std,\n",
    "                \"samples\": samples,\n",
    "                \"sample_dist\": sample_dist,\n",
    "                \"outside_1-std\": dist > out1,\n",
    "                \"outside_2-std\": dist > out2,\n",
    "            }\n",
    "\n",
    "            tracking[int(arr_i[-1])] = (level, cluster_num)  # Associates the new linkage id with the correct cluster\n",
    "\n",
    "            # If no left or right id, increment tracker to ensure new clusters get unique ids\n",
    "            if not left_id and not right_id:\n",
    "                cluster_tracking += 1\n",
    "            # Update clusters to include previously skipped levels\n",
    "            if left_id and left_id[0] + 1 != level:\n",
    "                clusters = self.fill_level(left_id, level, clusters)\n",
    "            if right_id and right_id[0] + 1 != level:\n",
    "                clusters = self.fill_level(right_id, level, clusters)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def fill_level(self, cluster_id, level, clusters):\n",
    "        new_level, cluster = cluster_id[0] + 1, cluster_id[1]\n",
    "        cluster_info = {\n",
    "            \"cluster_merged\": False,\n",
    "            \"count\": clusters[new_level - 1][cluster][\"count\"],\n",
    "            \"avg_dist\": clusters[new_level - 1][cluster][\"avg_dist\"],\n",
    "            \"dist_std\": clusters[new_level - 1][cluster][\"dist_std\"],\n",
    "            \"samples\": clusters[new_level - 1][cluster][\"samples\"],\n",
    "            \"sample_dist\": clusters[new_level - 1][cluster][\"sample_dist\"],\n",
    "            \"outside_1-std\": False,\n",
    "            \"outside_2-std\": False,\n",
    "        }\n",
    "        # Sets each level's cluster info if it does not exist\n",
    "        for level_id in range(level - 1, new_level - 2, -1):\n",
    "            clusters[level_id].setdefault(cluster, cluster_info)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def get_cluster_distances(self, clusters):\n",
    "        # this is the cluster distance matrix\n",
    "        cluster_matrix = np.full((self.last_merge_level, self.max_clusters, self.max_clusters), -1.0, dtype=np.float32)\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            if level < self.last_merge_level:\n",
    "                cluster_ids = sorted(cluster_set.keys())\n",
    "                for i, cluster_id in enumerate(cluster_ids):\n",
    "                    cluster_matrix[level, cluster_id, cluster_id] = clusters[level][cluster_id][\"avg_dist\"]\n",
    "                    for int_id in range(i + 1, len(cluster_ids)):\n",
    "                        compare_id = cluster_ids[int_id]\n",
    "                        sample_a = clusters[level][cluster_id][\"samples\"]\n",
    "                        sample_b = clusters[level][compare_id][\"samples\"]\n",
    "                        min_mat = self.sqdmat[np.ix_(sample_a, sample_b)].min()\n",
    "                        cluster_matrix[level, cluster_id, compare_id] = min_mat\n",
    "                        cluster_matrix[level, compare_id, cluster_id] = min_mat\n",
    "\n",
    "        return cluster_matrix\n",
    "\n",
    "    def calc_merge_indices(self, merge_mean, intra_max):\n",
    "        intra_max_uniques = np.unique(intra_max)\n",
    "        intra_log_values = np.log(intra_max_uniques)\n",
    "        two_std_all = intra_log_values.mean() + 2 * intra_log_values.std()\n",
    "        merge_value = np.log(merge_mean)\n",
    "        # Mask of indices we know we want to merge\n",
    "        desired_merge = merge_value < two_std_all\n",
    "\n",
    "        # List[Values] for indices we might want to merge\n",
    "        check = merge_value[~desired_merge]\n",
    "        # Check distance from value to 2 stds of all values\n",
    "        check = np.abs((check - two_std_all) / two_std_all)\n",
    "        # Mask List[Values < 1]\n",
    "        mask = check < 1\n",
    "        one_std_check = check[mask].mean() + check[mask].std()\n",
    "        # Mask of indices that should also be merged\n",
    "        mask2_vals = np.abs((merge_value - two_std_all) / two_std_all)\n",
    "        mask2 = mask2_vals < one_std_check\n",
    "        return np.logical_or(desired_merge, mask2)\n",
    "\n",
    "    def generate_merge_list(self, clusters: dict, cluster_matrix: np.ndarray):\n",
    "        \"\"\"\n",
    "        Runs through the clusters dictionary determining when clusters merge,\n",
    "        and how close are those clusters when they merge.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clusters:\n",
    "            A dictionary containing the original clusters information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        merge_clusters:\n",
    "            A dictionary with each clusters merge history\n",
    "        \"\"\"\n",
    "        intra_max = []\n",
    "        merge_mean = []\n",
    "        merge_list = []\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            for outer_cluster, cluster_info in cluster_set.items():\n",
    "                inner_cluster = cluster_info[\"cluster_merged\"]\n",
    "                if not inner_cluster:\n",
    "                    continue\n",
    "                # Extract necessary information\n",
    "                num_samples = len(cluster_info[\"samples\"])\n",
    "                out1 = cluster_info[\"outside_1-std\"]\n",
    "                out2 = cluster_info[\"outside_2-std\"]\n",
    "\n",
    "                # If outside 2-std or 1-std and larger than a minimum sized cluster, take the mean distance, else max\n",
    "                aggregate_func = (\n",
    "                    np.mean if out2 or (out1 and num_samples >= self.min_num_samples_per_cluster) else np.max\n",
    "                )\n",
    "\n",
    "                distances = cluster_matrix[:level, outer_cluster, inner_cluster]\n",
    "                intra_distance = cluster_matrix[:, outer_cluster, outer_cluster]\n",
    "                positive_mask = intra_distance >= 0\n",
    "                intra_filtered = intra_distance[positive_mask]\n",
    "\n",
    "                # TODO: Append now, take max over axis later?\n",
    "                intra_max.append(np.max(intra_filtered))\n",
    "                # Calculate the corresponding distance stats\n",
    "                distance_stats_arr = aggregate_func(distances)\n",
    "                merge_mean.append(distance_stats_arr)\n",
    "                merge_list.append([level, outer_cluster, inner_cluster])\n",
    "\n",
    "        all_merge_indices = self.calc_merge_indices(merge_mean=merge_mean, intra_max=intra_max)\n",
    "\n",
    "        for i, is_mergeable in enumerate(all_merge_indices):\n",
    "            merge_list[i].append(is_mergeable)\n",
    "\n",
    "        merge_list = sorted(merge_list, reverse=True)\n",
    "\n",
    "        return merge_list\n",
    "\n",
    "    def get_last_merge_levels(self, merge_list):\n",
    "        last_good_merge_levels = {}\n",
    "        for entry in merge_list:\n",
    "            level, outer_cluster, inner_cluster, status = entry\n",
    "            if not status:\n",
    "                if outer_cluster not in last_good_merge_levels:\n",
    "                    last_good_merge_levels[outer_cluster] = 1\n",
    "                if inner_cluster not in last_good_merge_levels:\n",
    "                    last_good_merge_levels[inner_cluster] = 1\n",
    "                if last_good_merge_levels[outer_cluster] > level:\n",
    "                    last_good_merge_levels[outer_cluster] = level - 1\n",
    "            else:\n",
    "                if outer_cluster in last_good_merge_levels:\n",
    "                    last_good_merge_levels[outer_cluster] = max(last_good_merge_levels[outer_cluster], level)\n",
    "        return last_good_merge_levels\n",
    "\n",
    "    def find_outliers(self, clusters, last_merge_levels):\n",
    "        \"\"\"\n",
    "        The clusters dictionary contains whether the added sample/cluster\n",
    "        was outside 1 standard deviation or outside 2 standard deviations.\n",
    "        last_merge_levels contains the last good merge for each cluster we care about\n",
    "        Using this information to determine when the sample was added to the cluster\n",
    "        and how far it was from the cluster when it was added\n",
    "\n",
    "        \"\"\"\n",
    "        outliers = []\n",
    "        possible_outliers = []\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            # cluster_ids = sorted(cluster_set.keys())\n",
    "            for cluster_id, cluster_info in cluster_set.items():\n",
    "                # Extract necessary information\n",
    "                samples = cluster_info[\"samples\"]\n",
    "                merged = cluster_info[\"cluster_merged\"]\n",
    "                out1 = cluster_info[\"outside_1-std\"]\n",
    "                out2 = cluster_info[\"outside_2-std\"]\n",
    "\n",
    "                if cluster_id in last_merge_levels and not merged:\n",
    "                    if level > last_merge_levels[cluster_id] and out2:\n",
    "                        outliers.append(samples[-1])\n",
    "                    elif (\n",
    "                        level > last_merge_levels[cluster_id]\n",
    "                        and out1\n",
    "                        and len(samples) >= self.min_num_samples_per_cluster\n",
    "                    ):\n",
    "                        possible_outliers.append(samples[-1])\n",
    "        return outliers, possible_outliers\n",
    "\n",
    "    def calc_duplicate_std(self, info, last_merge_levels):\n",
    "        dedup_std = []\n",
    "        additional_outliers = []\n",
    "        for cluster, level in last_merge_levels.items():\n",
    "            samples = info[level][cluster][\"samples\"]\n",
    "            if samples.shape[0] < self.min_num_samples_per_cluster:\n",
    "                additional_outliers.extend(samples.tolist())\n",
    "            else:\n",
    "                dedup_std.append(info[level][cluster][\"dist_std\"])\n",
    "        return dedup_std, additional_outliers\n",
    "\n",
    "    def find_duplicates(self, dedup_std_list):\n",
    "        diag_mask = np.ones_like(self.sqdmat, dtype=bool)\n",
    "        np.fill_diagonal(diag_mask, 0)\n",
    "        diag_mask = np.triu(diag_mask)\n",
    "\n",
    "        exact_mask = self.sqdmat < (np.mean(dedup_std_list) / 100)\n",
    "        exact_indices = np.nonzero(exact_mask & diag_mask)\n",
    "        exact_dedup = list(zip(exact_indices[0], exact_indices[1]))\n",
    "\n",
    "        possible_mask = self.sqdmat < np.mean(dedup_std_list)\n",
    "        possible_indices = np.nonzero(possible_mask & diag_mask & ~exact_mask)\n",
    "        possible_dedup = list(zip(possible_indices[0], possible_indices[1]))\n",
    "\n",
    "        return exact_dedup, possible_dedup\n",
    "\n",
    "    def run(self):\n",
    "        clusters_info_per_level = self.create_clusters()\n",
    "\n",
    "        if self.max_clusters > 1:\n",
    "            cluster_matrix = self.get_cluster_distances(clusters_info_per_level)\n",
    "            merge_list = self.generate_merge_list(clusters_info_per_level, cluster_matrix)\n",
    "            last_good_merge_levels = self.get_last_merge_levels(merge_list)\n",
    "        else:\n",
    "            last_good_merge_levels = {0: int(self.num_samples * 0.1)}\n",
    "\n",
    "        outliers, potential_outliers = self.find_outliers(clusters_info_per_level, last_good_merge_levels)\n",
    "        # duplicates_std, additional_outliers = self.calc_duplicate_std(clusters_info_per_level, last_good_merge_levels)\n",
    "\n",
    "        duplicates_std = []\n",
    "        additional_outliers = []\n",
    "        for cluster, level in last_good_merge_levels.items():\n",
    "            samples = clusters_info_per_level[level][cluster][\"samples\"]\n",
    "            if len(samples) < self.min_num_samples_per_cluster:\n",
    "                additional_outliers.extend(samples.tolist())\n",
    "            else:\n",
    "                duplicates_std.append(clusters_info_per_level[level][cluster][\"dist_std\"])\n",
    "\n",
    "        outliers.extend(additional_outliers)\n",
    "        duplicates, near_duplicates = self.find_duplicates(duplicates_std)\n",
    "\n",
    "        ret = {\n",
    "            \"outliers\": outliers,\n",
    "            \"potential_outliers\": potential_outliers,\n",
    "            \"duplicates\": duplicates,\n",
    "            \"near_duplicates\": near_duplicates,\n",
    "        }\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwds = {\"alpha\": 0.5, \"s\": 50, \"linewidths\": 0}\n",
    "\n",
    "# moons, _ = dsets.make_moons(n_samples=50, noise=0.1)\n",
    "blobs, _ = dsets.make_blobs(  # type: ignore\n",
    "    n_samples=100,\n",
    "    centers=[(-1.5, 1.8), (-1, 3), (0.8, 2.1), (2.8, 1.5), (2.5, 3.5)],  # type: ignore\n",
    "    cluster_std=0.3,\n",
    "    random_state=33,\n",
    ")\n",
    "# test_data = np.vstack([moons, blobs])\n",
    "test_data = blobs\n",
    "test_data[79] = test_data[24]\n",
    "test_data[63] = test_data[58] + 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaun's code for testing the clusterer\n",
    "def test_outliers(x):\n",
    "    assert len(x) == 6\n",
    "    for val in x:\n",
    "        assert val in [21, 6, 4, 71, 38, 11]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_potential_outliers(x):\n",
    "    assert len(x) == 5\n",
    "    for val in x:\n",
    "        assert val in [42, 48, 9, 1, 43]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_duplicates(x):\n",
    "    assert x == [(24, 79), (58, 63)]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_near_duplicates(x):\n",
    "    assert x == [\n",
    "        (8, 27),\n",
    "        (10, 65),\n",
    "        (16, 99),\n",
    "        (19, 64),\n",
    "        (22, 87),\n",
    "        (27, 29),\n",
    "        (33, 76),\n",
    "        (39, 55),\n",
    "        (40, 72),\n",
    "        (41, 62),\n",
    "        (80, 81),\n",
    "        (80, 93),\n",
    "        (81, 93),\n",
    "        (87, 95),\n",
    "    ]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def _recursive_dict_equality(d1: dict, d2: dict):\n",
    "    for k, v in d1.items():\n",
    "        if isinstance(v, dict):\n",
    "            _recursive_dict_equality(v, d2[k])\n",
    "        else:\n",
    "            x = np.array([v]).flatten()\n",
    "            y = np.array([d2[k]]).flatten()\n",
    "            assert all(x == y)\n",
    "\n",
    "\n",
    "def test_recursive_dicts(d1: dict, d2: dict):\n",
    "    _recursive_dict_equality(d1, d2)\n",
    "    print(\"Dict Equality: Passed\")\n",
    "\n",
    "\n",
    "def _recursive_list_equality(l1: list, l2: list):\n",
    "    assert len(l1) == len(l2)\n",
    "    for i1, i2 in zip(l1, l2):\n",
    "        if isinstance(i1, list):\n",
    "            _recursive_list_equality(i1, i2)\n",
    "        else:\n",
    "            assert i1 == i2\n",
    "\n",
    "\n",
    "def test_lists_equal(l1: list, l2: list):\n",
    "    _recursive_list_equality(l1, l2)\n",
    "    print(\"List Equality: Passed\")\n",
    "\n",
    "\n",
    "def test_matrix_equal(m1: np.ndarray, m2: np.ndarray):\n",
    "    np.testing.assert_array_equal(m1, m2)\n",
    "    print(\"Matrix Equality: Passed\")\n",
    "\n",
    "\n",
    "def run_tests(x):\n",
    "    test_outliers(x[\"outliers\"])\n",
    "    test_potential_outliers(x[\"potential_outliers\"])\n",
    "    test_duplicates(x[\"duplicates\"])\n",
    "    test_near_duplicates(x[\"near_duplicates\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import daml._internal.metrics.clustering as clustering\n",
    "\n",
    "reload(clustering)\n",
    "\n",
    "coo = clustering.Clusterer(test_data)\n",
    "coo.create_clusters()\n",
    "coo.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clusterer_new import Clusterer as Clusterer2\n",
    "\n",
    "clusterer = Clusterer(test_data)\n",
    "clusterer2 = Clusterer2(test_data)\n",
    "info = clusterer.create_clusters()\n",
    "info2 = clusterer2.create_clusters()\n",
    "test_recursive_dicts(info, info2)\n",
    "\n",
    "cluster_matrix = clusterer.get_cluster_distances(info)\n",
    "cluster_matrix2 = clusterer2.get_cluster_distances(info2)\n",
    "test_matrix_equal(cluster_matrix, cluster_matrix2)\n",
    "\n",
    "merge_levels = clusterer.get_merge_levels(info)\n",
    "# merge_levels2 = clusterer2.get_merge_levels(info2)\n",
    "# test_recursive_dicts(merge_levels, merge_levels2)\n",
    "\n",
    "merge_list = clusterer.generate_merge_list(merge_levels, cluster_matrix)\n",
    "merge_list2 = clusterer2.generate_merge_list(info2, cluster_matrix2)\n",
    "# Replaced test since \"status\" is a str vs bool for the two versions\n",
    "assert len(merge_list2) == len(merge_list)\n",
    "for l1, l2 in zip(merge_list, merge_list2):\n",
    "    assert l1[:3] == l2[:3]\n",
    "    assert (l1[-1] == \"merge\") == l2[-1]\n",
    "# test_lists_equal(merge_list, merge_list2)\n",
    "\n",
    "last_merge_levels = clusterer.get_last_merge_levels(merge_list)\n",
    "last_merge_levels2 = clusterer2.get_last_merge_levels(merge_list2)\n",
    "test_recursive_dicts(last_merge_levels, last_merge_levels2)\n",
    "\n",
    "outliers, potential_outliers = clusterer.find_outliers(info, last_merge_levels)\n",
    "outliers2, potential_outliers2 = clusterer2.find_outliers(info, last_merge_levels2)\n",
    "test_lists_equal(sorted(outliers), sorted(outliers2))\n",
    "test_lists_equal(potential_outliers, potential_outliers2)\n",
    "\n",
    "dedup_std = []\n",
    "for cluster, level in last_merge_levels.items():\n",
    "    level_cluster = info[level][cluster]\n",
    "    samples = level_cluster[\"samples\"]\n",
    "    if samples.shape[0] < clusterer.min_num_samples_per_cluster:\n",
    "        outliers.extend(samples.tolist())\n",
    "    else:\n",
    "        dedup_std.append(level_cluster[\"dist_std\"])\n",
    "\n",
    "dedup_std2 = []\n",
    "additional_outliers = []\n",
    "for cluster, level in last_merge_levels2.items():\n",
    "    samples = info[level][cluster][\"samples\"]\n",
    "    if samples.shape[0] < clusterer2.min_num_samples_per_cluster:\n",
    "        additional_outliers.extend(samples.tolist())\n",
    "    else:\n",
    "        dedup_std2.append(info[level][cluster][\"dist_std\"])\n",
    "\n",
    "duplicates, near_duplicates = clusterer.find_duplicates(dedup_std)\n",
    "# dedup_std2 = clusterer2.calc_du\n",
    "duplicates2, near_duplicates2 = clusterer2.find_duplicates(dedup_std2)\n",
    "test_lists_equal(duplicates, duplicates2)\n",
    "test_lists_equal(near_duplicates, near_duplicates2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>>>\\tCreate Clusters\\t<<<<<\")\n",
    "%timeit clusterer.create_clusters()\n",
    "%timeit clusterer2.create_clusters()\n",
    "print(\">>>>>\\tCluster Distances\\t<<<<<\")\n",
    "%timeit clusterer.get_cluster_distances(info)\n",
    "%timeit clusterer2.get_cluster_distances(info2)\n",
    "print(\">>>>>\\tMerge Levels\\t<<<<<\")\n",
    "%timeit clusterer.get_merge_levels(info)\n",
    "# %timeit clusterer2.get_merge_levels(info2)\n",
    "print(\">>>>>\\tMerge List\\t<<<<<\")\n",
    "%timeit clusterer.generate_merge_list(merge_levels, cluster_matrix)\n",
    "%timeit clusterer2.generate_merge_list(info2, cluster_matrix2)\n",
    "print(\">>>>>\\tLast Merge Levels\\t<<<<<\")\n",
    "%timeit clusterer.get_last_merge_levels(merge_list)\n",
    "%timeit clusterer2.get_last_merge_levels(merge_list2)\n",
    "print(\">>>>>\\tOutliers\\t<<<<<\")\n",
    "%timeit clusterer.find_outliers(info, last_merge_levels)\n",
    "%timeit clusterer2.find_outliers(info2, last_merge_levels2)\n",
    "print(\">>>>>\\tDuplicates\\t<<<<<\")\n",
    "%timeit clusterer.find_duplicates(dedup_std)\n",
    "%timeit clusterer2.find_duplicates(dedup_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "clusterer.create_clusters()\n",
    "clusterer.get_cluster_distances(info)\n",
    "clusterer.get_merge_levels(info)\n",
    "clusterer.generate_merge_list(merge_levels, cluster_matrix2)\n",
    "clusterer.get_last_merge_levels(merge_list)\n",
    "clusterer.find_outliers(info, last_merge_levels2)\n",
    "clusterer.find_duplicates(dedup_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "clusterer2.create_clusters()\n",
    "clusterer2.get_cluster_distances(info2)\n",
    "clusterer2.generate_merge_list(info2, cluster_matrix2)\n",
    "clusterer2.get_last_merge_levels(merge_list2)\n",
    "clusterer2.find_outliers(info2, last_merge_levels2)\n",
    "clusterer2.find_duplicates(dedup_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
